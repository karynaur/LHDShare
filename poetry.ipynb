{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poetry.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d7e1f92ab4c43d4bb7b0ae710830ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_43e4357c78cb4dbba59a5aafc76adbb6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_65b23f8063b047eeb441e3762ea526cf",
              "IPY_MODEL_18331af0256a47f587c7b171690a8956"
            ]
          }
        },
        "43e4357c78cb4dbba59a5aafc76adbb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65b23f8063b047eeb441e3762ea526cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e92cfe3f02554ca4bbbf732bee78874e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85274d5c79954939b2a4f7bcf808383e"
          }
        },
        "18331af0256a47f587c7b171690a8956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_63e8c35d877c4ce5aa4c3d55625dcfd0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 5.79MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a63baec28ac4bbe8f6c3a612c595931"
          }
        },
        "e92cfe3f02554ca4bbbf732bee78874e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85274d5c79954939b2a4f7bcf808383e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63e8c35d877c4ce5aa4c3d55625dcfd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a63baec28ac4bbe8f6c3a612c595931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2dadeba3edc541e1946d68934d340096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_41295ea7e6a347cd9b705cac2ed4ec5b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7ba45318b01146ba9b0ffbb11e6c5663",
              "IPY_MODEL_6fb5f07e5ce24ebab73eebbf35e57a2b"
            ]
          }
        },
        "41295ea7e6a347cd9b705cac2ed4ec5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ba45318b01146ba9b0ffbb11e6c5663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b5fddaa70df640afa98d59dd676585fe",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_360bbafd091642a8a272005745cd71c8"
          }
        },
        "6fb5f07e5ce24ebab73eebbf35e57a2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6678efe8913b47a187a4215051c6fa02",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 732kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d231cde00e484e5ebe50f96e714be191"
          }
        },
        "b5fddaa70df640afa98d59dd676585fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "360bbafd091642a8a272005745cd71c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6678efe8913b47a187a4215051c6fa02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d231cde00e484e5ebe50f96e714be191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84cc8cbe1b914d24a94e1c1c0c3cde2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_80e22b7a1a8f4ef9967d656f85e9ea26",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6e534fbfeefa4ddaae5a1818e466dfd6",
              "IPY_MODEL_05a293e339b04247bbd4fc40ce54426b"
            ]
          }
        },
        "80e22b7a1a8f4ef9967d656f85e9ea26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6e534fbfeefa4ddaae5a1818e466dfd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1613f5ce378846a888154a5651d71c29",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 665,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 665,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9677cc4d3faa40c18699edeb4982162e"
          }
        },
        "05a293e339b04247bbd4fc40ce54426b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_83cd2e653bab441ba23763d599bf5220",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 665/665 [00:14&lt;00:00, 44.5B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_01f5eb0a22de4d7bb6a5b5bc0d52b658"
          }
        },
        "1613f5ce378846a888154a5651d71c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9677cc4d3faa40c18699edeb4982162e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83cd2e653bab441ba23763d599bf5220": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01f5eb0a22de4d7bb6a5b5bc0d52b658": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29dfe44d911e4c05b3c230b06b2f764c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4cbe5b56f3c0442f8e83f7304705ef03",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f725c70dd6ea421e901effcfb18e83ae",
              "IPY_MODEL_fc8ddee656e642fb80b44a0ece763bde"
            ]
          }
        },
        "4cbe5b56f3c0442f8e83f7304705ef03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f725c70dd6ea421e901effcfb18e83ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4cc38686b88d4be9917632baff559b57",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 548118077,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 548118077,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_be406e942ba34cca8161e86625bde862"
          }
        },
        "fc8ddee656e642fb80b44a0ece763bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f70f3118e5e2475b97ea0044c3a511fe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:10&lt;00:00, 51.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29466875352f4d0a81f197f1be96b5c1"
          }
        },
        "4cc38686b88d4be9917632baff559b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "be406e942ba34cca8161e86625bde862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f70f3118e5e2475b97ea0044c3a511fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29466875352f4d0a81f197f1be96b5c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WYSMerd2TwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd435187-9160-4b9c-e275-73442d18956d"
      },
      "source": [
        "!git clone https://github.com/summerstay/true_poetry.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'true_poetry'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 233 (delta 15), reused 2 (delta 0), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (233/233), 25.86 MiB | 30.06 MiB/s, done.\n",
            "Resolving deltas: 100% (120/120), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYSF3kDJkPJL",
        "outputId": "490e6477-b1aa-4a0b-8bf1-870fcf0b3bff"
      },
      "source": [
        "%cd true_poetry/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/true_poetry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "5_Gw7R_IkYey",
        "outputId": "1c574a77-9c06-4636-c499-d75ce601c0d3"
      },
      "source": [
        "!pip install transformers==3.0.0\n",
        "!pip install win_unicode_console"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 20.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/82/0e82a95bd9db2b32569500cc1bb47aa7c4e0f57aa5e35cceba414096917b/tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 54.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.0.43)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 39.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
            "Installing collected packages: tokenizers, sentencepiece, transformers\n",
            "  Found existing installation: tokenizers 0.10.1\n",
            "    Uninstalling tokenizers-0.10.1:\n",
            "      Successfully uninstalled tokenizers-0.10.1\n",
            "  Found existing installation: transformers 4.4.2\n",
            "    Uninstalling transformers-4.4.2:\n",
            "      Successfully uninstalled transformers-4.4.2\n",
            "Successfully installed sentencepiece-0.1.95 tokenizers-0.8.0rc4 transformers-3.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: win_unicode_console in /usr/local/lib/python3.7/dist-packages (0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3d7e1f92ab4c43d4bb7b0ae710830ca8",
            "43e4357c78cb4dbba59a5aafc76adbb6",
            "65b23f8063b047eeb441e3762ea526cf",
            "18331af0256a47f587c7b171690a8956",
            "e92cfe3f02554ca4bbbf732bee78874e",
            "85274d5c79954939b2a4f7bcf808383e",
            "63e8c35d877c4ce5aa4c3d55625dcfd0",
            "8a63baec28ac4bbe8f6c3a612c595931",
            "2dadeba3edc541e1946d68934d340096",
            "41295ea7e6a347cd9b705cac2ed4ec5b",
            "7ba45318b01146ba9b0ffbb11e6c5663",
            "6fb5f07e5ce24ebab73eebbf35e57a2b",
            "b5fddaa70df640afa98d59dd676585fe",
            "360bbafd091642a8a272005745cd71c8",
            "6678efe8913b47a187a4215051c6fa02",
            "d231cde00e484e5ebe50f96e714be191",
            "84cc8cbe1b914d24a94e1c1c0c3cde2f",
            "80e22b7a1a8f4ef9967d656f85e9ea26",
            "6e534fbfeefa4ddaae5a1818e466dfd6",
            "05a293e339b04247bbd4fc40ce54426b",
            "1613f5ce378846a888154a5651d71c29",
            "9677cc4d3faa40c18699edeb4982162e",
            "83cd2e653bab441ba23763d599bf5220",
            "01f5eb0a22de4d7bb6a5b5bc0d52b658",
            "29dfe44d911e4c05b3c230b06b2f764c",
            "4cbe5b56f3c0442f8e83f7304705ef03",
            "f725c70dd6ea421e901effcfb18e83ae",
            "fc8ddee656e642fb80b44a0ece763bde",
            "4cc38686b88d4be9917632baff559b57",
            "be406e942ba34cca8161e86625bde862",
            "f70f3118e5e2475b97ea0044c3a511fe",
            "29466875352f4d0a81f197f1be96b5c1"
          ]
        },
        "id": "sVjQfuUekWB4",
        "outputId": "2e4c1b08-930c-45f7-f929-759a1a74c0d5"
      },
      "source": [
        "import string as str\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch.nn.functional as F\n",
        "from random import randint, seed\n",
        "import math\n",
        "import pickle \n",
        "import re\n",
        "\n",
        "# TO DO: force last token to end a sentence\n",
        "# revise previous lines if we can't find a good rhyme\n",
        "\n",
        "punctuation = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 220, 352, 357, 362, 366, 405, 438, 486, 492, 513, 526, 532, 553, 580, 604, 642, 650, 657, 678, 685, 705, 718, 720, 737, 764, 767, 796, 807, 828, 830, 834, 837, 838, 855, 860, 930, 939, 940, 982, 986, 1003, 1058, 1065, 1105, 1106, 1120, 1129, 1157, 1160, 1174, 1220, 1222, 1238, 1248, 1264, 1267, 1270, 1279, 1298, 1303, 1314, 1315, 1343, 1367, 1377, 1378, 1391, 1415, 1421, 1427, 1433, 1467, 1478, 1485, 1495, 1507, 1511, 1539, 1542, 1558, 1584, 1594, 1596, 1600, 1635, 1679, 1701, 1731, 1776, 1782, 1783, 1795, 1802, 1821, 1828, 1853, 1875, 1899, 1911, 1946, 1954, 1959, 1983, 1987, 2014, 2026, 2075, 2078, 2079, 2091, 2109, 2154, 2162, 2167, 2177, 2211, 2231, 2235, 2242, 2310, 2319, 2321, 2327, 2361, 2388, 2404, 2414, 2425, 2430, 2466, 2474, 2481, 2488, 2534, 2548, 2559, 2579, 2598, 2599, 2602, 2608, 2623, 2624, 2625, 2637, 2644, 2670, 2681, 2682, 2713, 2718, 2757, 2780, 2791, 2808, 2813, 2816, 2857, 2864, 2919, 2920, 2931, 2996, 2998, 2999, 3023, 3050, 3064, 3070, 3104, 3126, 3132, 3134, 3228, 3256, 3261, 3270, 3312, 3324, 3365, 3373, 3388, 3419, 3439, 3459, 3467, 3510, 3548, 3553, 3556, 3559, 3571, 3648, 3682, 3693, 3695, 3712, 3717, 3720, 3784, 3829, 3865, 3880, 3901, 3933, 3980, 4008, 4019, 4032, 4051, 4059, 4064, 4083, 4089, 4101, 4153, 4181, 4211, 4242, 4275, 4304, 4309, 4310, 4317, 4343, 4349, 4353, 4357, 4407, 4458, 4521, 4524, 4531, 4557, 4570, 4600, 4613, 4626, 4747, 4751, 4761, 4764, 4770, 4790, 4793, 4808, 4841, 4846, 4869, 4880, 4895, 4907, 4943, 4967, 4974, 5014, 5066, 5075, 5125, 5145, 5214, 5218, 5237, 5299, 5304, 5320, 5323, 5332, 5333, 5433, 5441, 5472, 5512, 5534, 5539, 5598, 5607, 5619, 5633, 5705, 5769, 5774, 5816, 5824, 5846, 5855, 5867, 5878, 5892, 5946, 5974, 5996, 5999, 6052, 6073, 6135, 6200, 6244, 6298, 6303, 6329, 6337, 6390, 6420, 6469, 6624, 6640, 6659, 6739, 6740, 6852, 6885, 6927, 6957, 6999, 7029, 7061, 7131, 7169, 7175, 7192, 7198, 7203, 7225, 7265, 7337, 7358, 7359, 7388, 7410, 7441, 7479, 7499, 7559, 7600, 7618, 7632, 7643, 7665, 7724, 7769, 7795, 7804, 7816, 7863, 7874, 7879, 7904, 7908, 7930, 7982, 8054, 8069, 8093, 8133, 8162, 8172, 8183, 8190, 8235, 8257, 8269, 8275, 8298, 8309, 8348, 8351, 8412, 8454, 8487, 8541, 8576, 8614, 8628, 8644, 8646, 8684, 8699, 8702, 8728, 8735, 8753, 8762, 8784, 8854, 8864, 8870, 8915, 8949, 8964, 8973, 9031, 9063, 9130, 9162, 9166, 9193, 9225, 9313, 9415, 9466, 9507, 9508, 9609, 9656, 9661, 9698, 9705, 9768, 9773, 9783, 9796, 9804, 9805, 9816, 9832, 9849, 9879, 9907, 9919, 9959, 10048, 10052, 10053, 10083, 10091, 10097, 10111, 10148, 10163, 10185, 10190, 10221, 10232, 10249, 10261, 10333, 10354, 10460, 10495, 10531, 10535, 10541, 10563, 10612, 10779, 10786, 11024, 11037, 11074, 11097, 11104, 11139, 11207, 11208, 11245, 11323, 11405, 11420, 11442, 11445, 11470, 11485, 11496, 11502, 11509, 11528, 11537, 11546, 11592, 11593, 11623, 11639, 11645, 11709, 11757, 11785, 11900, 11907, 11919, 12095, 12113, 12122, 12131, 12179, 12195, 12240, 12248, 12279, 12340, 12359, 12404, 12429, 12713, 12726, 12762, 12813, 12825, 12844, 12863, 12865, 12877, 12878, 12923, 12952, 12962, 13018, 13037, 13108, 13130, 13151, 13163, 13219, 13330, 13343, 13348, 13374, 13381, 13412, 13426, 13454, 13464, 13498, 13521, 13531, 13538, 13539, 13540, 13557, 13679, 13702, 13803, 13841, 13896, 13979, 13984, 14004, 14030, 14062, 14079, 14198, 14223, 14280, 14315, 14373, 14436, 14454, 14468, 14489, 14512, 14585, 14610, 14631, 14656, 14686, 14692, 14726, 14745, 14804, 14808, 14877, 14956, 14980, 15089, 15090, 15116, 15136, 15143, 15179, 15187, 15197, 15211, 15231, 15259, 15277, 15306, 15341, 15349, 15363, 15377, 15408, 15426, 15437, 15473, 15495, 15506, 15524, 15533, 15589, 15629, 15674, 15696, 15711, 15724, 15729, 15761, 15801, 15853, 15885, 15886, 15897, 15904, 15913, 15920, 15931, 15963, 15982, 16003, 16078, 16088, 16101, 16102, 16150, 16151, 16193, 16208, 16226, 16236, 16243, 16315, 16317, 16327, 16382, 16410, 16450, 16471, 16489, 16529, 16562, 16589, 16616, 16626, 16677, 16679, 16725, 16763, 16791, 16792, 16799, 16817, 16822, 16942, 16945, 16994, 17020, 17031, 17032, 17059, 17174, 17202, 17241, 17279, 17318, 17342, 17405, 17414, 17426, 17430, 17464, 17477, 17501, 17544, 17553, 17569, 17572, 17575, 17635, 17643, 17657, 17672, 17729, 17759, 17816, 17817, 17827, 17885, 17912, 17971, 18005, 18083, 18109, 18112, 18125, 18161, 18182, 18189, 18237, 18294, 18298, 18376, 18395, 18444, 18458, 18477, 18500, 18523, 18604, 18638, 18693, 18741, 18742, 18781, 18823, 18897, 18938, 18946, 19004, 19035, 19038, 19039, 19048, 19060, 19104, 19153, 19203, 19214, 19244, 19322, 19342, 19351, 19355, 19409, 19415, 19420, 19424, 19427, 19442, 19504, 19510, 19529, 19570, 19571, 19588, 19622, 19629, 19683, 19707, 19708, 19710, 19755, 19779, 19782, 19841, 19880, 19884, 19891, 19924, 19953, 19969, 19990, 20004, 20007, 20033, 20064, 20107, 20167, 20198, 20219, 20224, 20233, 20248, 20262, 20299, 20343, 20356, 20368, 20370, 20379, 20416, 20479, 20483, 20510, 20520, 20548, 20598, 20662, 20666, 20679, 20708, 20740, 20789, 20809, 20924, 20943, 20959, 20964, 20986, 20995, 21017, 21033, 21056, 21113, 21139, 21148, 21215, 21261, 21268, 21273, 21288, 21315, 21355, 21387, 21395, 21409, 21431, 21489, 21495, 21498, 21503, 21526, 21536, 21577, 21599, 21601, 21626, 21643, 21652, 21709, 21719, 21734, 21737, 21738, 21747, 21761, 21777, 21794, 21807, 21844, 21895, 21908, 21912, 21940, 22039, 22042, 22047, 22131, 22133, 22135, 22136, 22148, 22169, 22172, 22186, 22219, 22241, 22243, 22288, 22291, 22305, 22318, 22330, 22337, 22345, 22352, 22369, 22370, 22413, 22416, 22446, 22458, 22492, 22515, 22538, 22544, 22567, 22572, 22579, 22613, 22626, 22666, 22675, 22709, 22717, 22725, 22730, 22745, 22759, 22784, 22799, 22800, 22831, 22842, 22855, 22857, 22883, 22909, 22913, 22914, 22935, 22951, 22955, 22980, 22985, 22986, 22995, 22996, 23029, 23031, 23045, 23055, 23120, 23134, 23148, 23188, 23193, 23195, 23234, 23237, 23313, 23330, 23336, 23344, 23349, 23362, 23378, 23451, 23460, 23487, 23516, 23539, 23601, 23628, 23664, 23666, 23679, 23721, 23726, 23734, 23753, 23756, 23785, 23815, 23846, 23847, 23859, 23871, 23884, 23906, 23924, 23926, 23984, 24018, 24022, 24036, 24038, 24041, 24045, 24063, 24096, 24136, 24137, 24168, 24200, 24214, 24217, 24235, 24293, 24294, 24305, 24309, 24339, 24356, 24369, 24403, 24409, 24414, 24457, 24465, 24529, 24555, 24591, 24598, 24618, 24620, 24648, 24652, 24669, 24693, 24718, 24760, 24793, 24839, 24840, 24844, 24848, 24894, 24909, 24938, 24940, 24943, 24970, 24977, 24991, 25022, 25054, 25061, 25090, 25096, 25106, 25113, 25128, 25150, 25177, 25181, 25190, 25191, 25226, 25240, 25248, 25257, 25264, 25270, 25272, 25295, 25307, 25325, 25326, 25399, 25429, 25475, 25500, 25508, 25540, 25597, 25600, 25609, 25643, 25644, 25645, 25658, 25666, 25667, 25674, 25698, 25707, 25710, 25719, 25764, 25780, 25787, 25816, 25829, 25836, 25838, 25859, 25870, 25887, 25915, 25947, 25948, 25964, 25970, 25998, 26007, 26033, 26050, 26063, 26073, 26076, 26115, 26118, 26143, 26171, 26200, 26214, 26250, 26259, 26276, 26279, 26290, 26352, 26358, 26398, 26422, 26427, 26429, 26481, 26488, 26492, 26513, 26514, 26525, 26539, 26561, 26582, 26598, 26607, 26660, 26700, 26704, 26709, 26717, 26753, 26780, 26793, 26826, 26833, 26866, 26867, 26881, 26894, 26895, 26912, 26933, 26937, 26956, 26989, 27007, 27019, 27033, 27037, 27057, 27071, 27097, 27121, 27137, 27156, 27191, 27192, 27193, 27203, 27211, 27228, 27230, 27246, 27253, 27260, 27267, 27277, 27301, 27310, 27326, 27367, 27368, 27371, 27408, 27412, 27422, 27444, 27493, 27514, 27550, 27551, 27559, 27584, 27613, 27621, 27641, 27649, 27653, 27691, 27693, 27696, 27712, 27720, 27728, 27754, 27778, 27790, 27791, 27795, 27800, 27824, 27829, 27877, 27896, 27936, 27937, 27956, 27970, 27972, 27988, 28011, 28013, 28017, 28041, 28054, 28072, 28104, 28112, 28119, 28174, 28200, 28256, 28262, 28264, 28265, 28277, 28296, 28324, 28358, 28362, 28369, 28401, 28404, 28460, 28481, 28551, 28555, 28560, 28567, 28581, 28592, 28598, 28645, 28658, 28669, 28676, 28684, 28687, 28688, 28694, 28714, 28725, 28727, 28771, 28815, 28817, 28857, 28872, 28878, 28896, 28933, 28947, 28955, 28977, 28978, 28988, 29001, 29006, 29022, 29041, 29059, 29088, 29101, 29110, 29113, 29119, 29143, 29159, 29164, 29173, 29211, 29217, 29225, 29226, 29228, 29279, 29300, 29326, 29331, 29334, 29335, 29342, 29343, 29414, 29416, 29513, 29524, 29558, 29565, 29568, 29577, 29626, 29637, 29653, 29691, 29694, 29703, 29720, 29769, 29795, 29796, 29807, 29847, 29865, 29903, 29953, 29994, 30005, 30057, 30072, 30109, 30110, 30120, 30123, 30138, 30179, 30206, 30272, 30273, 30290, 30299, 30336, 30368, 30420, 30435, 30453, 30460, 30478, 30483, 30484, 30487, 30505, 30543, 30557, 30607, 30610, 30629, 30695, 30704, 30727, 30743, 30763, 30803, 30823, 30827, 30863, 30866, 30924, 30934, 30960, 30986, 30989, 30992, 30995, 31009, 31010, 31011, 31020, 31027, 31034, 31046, 31051, 31064, 31102, 31113, 31115, 31128, 31161, 31175, 31211, 31360, 31380, 31386, 31418, 31478, 31495, 31496, 31503, 31510, 31520, 31552, 31566, 31575, 31654, 31666, 31672, 31675, 31697, 31714, 31751, 31773, 31794, 31820, 31883, 31911, 31916, 31936, 31938, 31952, 31953, 31980, 31982, 32047, 32056, 32059, 32062, 32066, 32090, 32092, 32105, 32114, 32118, 32128, 32148, 32158, 32182, 32190, 32196, 32203, 32215, 32216, 32220, 32239, 32284, 32320, 32321, 32382, 32406, 32417, 32459, 32471, 32501, 32509, 32531, 32535, 32544, 32568, 32576, 32583, 32590, 32591, 32614, 32624, 32637, 32642, 32647, 32747, 32756, 32759, 32811, 32817, 32843, 32865, 32869, 32883, 32921, 32941, 32996, 33015, 33028, 33032, 33042, 33047, 33057, 33116, 33121, 33151, 33153, 33160, 33172, 33206, 33223, 33250, 33283, 33289, 33300, 33319, 33372, 33394, 33400, 33409, 33438, 33448, 33459, 33470, 33490, 33507, 33529, 33535, 33548, 33551, 33580, 33581, 33618, 33638, 33646, 33660, 33690, 33698, 33717, 33747, 33759, 33761, 33781, 33797, 33808, 33809, 33813, 33879, 33882, 33916, 33924, 33942, 33963, 33981, 33994, 34013, 34044, 34085, 34091, 34107, 34125, 34131, 34135, 34137, 34155, 34159, 34171, 34206, 34215, 34229, 34251, 34256, 34287, 34294, 34323, 34353, 34373, 34385, 34400, 34427, 34463, 34465, 34483, 34489, 34507, 34516, 34583, 34598, 34604, 34607, 34617, 34620, 34625, 34626, 34635, 34713, 34716, 34729, 34741, 34758, 34770, 34772, 34801, 34808, 34825, 34865, 34913, 34938, 34949, 34951, 35005, 35038, 35090, 35124, 35126, 35133, 35145, 35148, 35150, 35175, 35195, 35218, 35264, 35273, 35307, 35343, 35360, 35369, 35378, 35379, 35384, 35402, 35404, 35411, 35419, 35430, 35435, 35447, 35494, 35500, 35514, 35534, 35540, 35549, 35592, 35617, 35625, 35638, 35642, 35656, 35665, 35667, 35709, 35713, 35745, 35751, 35768, 35793, 35809, 35844, 35890, 35897, 35916, 35922, 35937, 35944, 35978, 35989, 36006, 36042, 36058, 36088, 36094, 36100, 36117, 36141, 36147, 36150, 36189, 36203, 36243, 36244, 36260, 36330, 36338, 36445, 36453, 36463, 36475, 36490, 36521, 36561, 36563, 36565, 36566, 36625, 36626, 36629, 36641, 36657, 36658, 36676, 36678, 36680, 36720, 36737, 36786, 36792, 36796, 36809, 36864, 36879, 36911, 36917, 36928, 36937, 36959, 36966, 36993, 37082, 37128, 37144, 37160, 37166, 37187, 37224, 37227, 37250, 37255, 37272, 37283, 37290, 37309, 37364, 37381, 37397, 37404, 37405, 37452, 37466, 37498, 37517, 37528, 37547, 37563, 37576, 37601, 37633, 37637, 37667, 37674, 37680, 37688, 37710, 37730, 37737, 37747, 37750, 37781, 37804, 37811, 37831, 37841, 37856, 37864, 37867, 37913, 37922, 37950, 37967, 37981, 37988, 37991, 38016, 38055, 38056, 38073, 38089, 38093, 38107, 38108, 38123, 38147, 38155, 38158, 38165, 38172, 38190, 38205, 38210, 38214, 38219, 38249, 38314, 38326, 38339, 38362, 38369, 38377, 38380, 38381, 38384, 38391, 38430, 38431, 38446, 38449, 38472, 38503, 38508, 38525, 38547, 38549, 38565, 38569, 38595, 38605, 38612, 38634, 38652, 38703, 38721, 38783, 38819, 38831, 38850, 38867, 38892, 38902, 38905, 38907, 38956, 39064, 39084, 39088, 39093, 39101, 39103, 39111, 39115, 39118, 39121, 39132, 39135, 39166, 39174, 39188, 39195, 39226, 39251, 39254, 39260, 39277, 39280, 39320, 39322, 39357, 39380, 39397, 39434, 39449, 39466, 39506, 39509, 39570, 39595, 39647, 39658, 39667, 39697, 39710, 39761, 39763, 39768, 39850, 39861, 39864, 39882, 39885, 39893, 39923, 39925, 39937, 39997, 40022, 40035, 40064, 40090, 40103, 40111, 40149, 40173, 40179, 40215, 40220, 40248, 40256, 40264, 40271, 40278, 40286, 40350, 40353, 40384, 40385, 40393, 40400, 40401, 40403, 40417, 40427, 40454, 40463, 40484, 40486, 40523, 40538, 40554, 40585, 40639, 40643, 40652, 40654, 40660, 40675, 40703, 40719, 40720, 40736, 40754, 40761, 40791, 40828, 40839, 40873, 40884, 41019, 41023, 41052, 41060, 41103, 41172, 41208, 41234, 41235, 41241, 41247, 41263, 41287, 41289, 41290, 41292, 41322, 41349, 41417, 41423, 41424, 41435, 41436, 41441, 41507, 41531, 41544, 41561, 41569, 41573, 41580, 41583, 41612, 41625, 41647, 41655, 41706, 41707, 41717, 41734, 41739, 41761, 41810, 41813, 41820, 41832, 41853, 41874, 41879, 41888, 41906, 41922, 41931, 41934, 41948, 41977, 42018, 42032, 42060, 42117, 42141, 42163, 42199, 42210, 42215, 42224, 42240, 42246, 42250, 42294, 42303, 42313, 42321, 42334, 42363, 42444, 42479, 42489, 42496, 42501, 42520, 42534, 42535, 42548, 42622, 42638, 42669, 42671, 42691, 42716, 42720, 42744, 42751, 42752, 42759, 42780, 42785, 42802, 42819, 42830, 42875, 42877, 42911, 42924, 42944, 42947, 42980, 43019, 43053, 43054, 43134, 43147, 43155, 43179, 43184, 43193, 43234, 43239, 43240, 43284, 43292, 43313, 43336, 43356, 43364, 43367, 43379, 43434, 43452, 43489, 43509, 43526, 43550, 43564, 43571, 43587, 43610, 43634, 43637, 43641, 43661, 43665, 43686, 43690, 43697, 43704, 43722, 43734, 43785, 43798, 43801, 43825, 43839, 43864, 43916, 43918, 43922, 43927, 43950, 43977, 44063, 44084, 44085, 44087, 44093, 44103, 44104, 44167, 44169, 44183, 44212, 44214, 44215, 44218, 44227, 44230, 44233, 44274, 44300, 44318, 44341, 44361, 44367, 44386, 44388, 44417, 44427, 44435, 44438, 44465, 44468, 44505, 44541, 44550, 44552, 44578, 44586, 44587, 44613, 44617, 44622, 44626, 44627, 44646, 44648, 44673, 44675, 44688, 44698, 44713, 44717, 44729, 44750, 44785, 44807, 44808, 44821, 44825, 44826, 44856, 44912, 44926, 44928, 44966, 44969, 44980, 44994, 45021, 45039, 45063, 45068, 45088, 45095, 45144, 45151, 45160, 45191, 45192, 45210, 45214, 45271, 45278, 45297, 45299, 45302, 45310, 45326, 45331, 45337, 45340, 45345, 45385, 45403, 45418, 45422, 45432, 45434, 45438, 45439, 45440, 45455, 45469, 45473, 45491, 45537, 45598, 45600, 45601, 45611, 45620, 45719, 45720, 45722, 45734, 45758, 45791, 45839, 45881, 45900, 45937, 45959, 45969, 45987, 46044, 46068, 46083, 46096, 46110, 46111, 46121, 46239, 46244, 46249, 46250, 46302, 46328, 46351, 46352, 46393, 46396, 46402, 46424, 46425, 46435, 46438, 46444, 46477, 46491, 46519, 46556, 46570, 46572, 46581, 46588, 46589, 46618, 46633, 46636, 46660, 46712, 46720, 46723, 46752, 46761, 46815, 46821, 46839, 46841, 46866, 46871, 46872, 46899, 46900, 46904, 46939, 46951, 46956, 46957, 47007, 47026, 47072, 47082, 47101, 47106, 47113, 47159, 47175, 47182, 47197, 47202, 47226, 47232, 47233, 47235, 47253, 47282, 47308, 47325, 47338, 47343, 47372, 47396, 47407, 47448, 47454, 47465, 47493, 47505, 47512, 47521, 47527, 47540, 47567, 47576, 47580, 47582, 47671, 47679, 47682, 47705, 47715, 47744, 47760, 47762, 47784, 47785, 47795, 47801, 47835, 47838, 47915, 47932, 47934, 47936, 47941, 47946, 47996, 48000, 48057, 48065, 48069, 48082, 48096, 48104, 48132, 48136, 48156, 48170, 48173, 48185, 48193, 48194, 48200, 48207, 48219, 48220, 48246, 48250, 48252, 48284, 48290, 48340, 48341, 48365, 48372, 48391, 48443, 48475, 48524, 48527, 48528, 48529, 48531, 48548, 48555, 48564, 48581, 48597, 48600, 48602, 48609, 48630, 48634, 48638, 48645, 48655, 48683, 48688, 48712, 48724, 48725, 48758, 48768, 48774, 48777, 48868, 48874, 48882, 48889, 48891, 48894, 48908, 48952, 48964, 48999, 49020, 49029, 49051, 49074, 49082, 49087, 49125, 49129, 49146, 49150, 49211, 49231, 49234, 49259, 49287, 49294, 49296, 49327, 49351, 49352, 49356, 49388, 49429, 49447, 49489, 49503, 49517, 49539, 49541, 49542, 49545, 49557, 49561, 49563, 49584, 49616, 49633, 49641, 49649, 49658, 49669, 49682, 49689, 49703, 49704, 49721, 49778, 49803, 49814, 49841, 49856, 49888, 49934, 49946, 49954, 49959, 49989, 49995, 50038, 50049, 50055, 50080, 50113, 50119, 50138, 50148, 50150, 50154, 50155, 50165, 50184, 50205, 50242, 50247}\n",
        "acceptable_punctuation = {13, 0, 11, 30, 25, 26, 6, 1, 7, 8, 438, 12, 705, 357,1267,366,705,1377,220,764}\n",
        "end_punctuation ={0,13,11,26,30}\n",
        "class Struct:\n",
        "    pass\n",
        "params = Struct()\n",
        "params.rhyme_set_size = 20  # if a word will later be rhymed with, at least this many rhyming words must exist\n",
        "params.probability_threshold = .00005 # a token must have at least this probability of being the next token. .02 = better quality but slower; .0005 = worse quality but faster. If you use one-syllable suppression, these limits should be more like .005 and .00005.\n",
        "params.line_probability_threshold = 0 #total multiplied out probability of entire line of tokens can be no lower than this. 0 means this isn't being used.\n",
        "params.ultimate_expansion = 1000 # no more than this many words will be tried as the last syllable for any previous phrase\n",
        "params.penultimate_expansion = 10 # no more than this many words will be tried for the next to last syllable for any previous phrase\n",
        "params.other_expansion = 10 # no more than this many words will be tried for the second through next to last syllables of any phrase\n",
        "params.random_seed = 28 # if the seed and prompt are the same, the poem will be the same\n",
        "params.line_end_punctuation_constraint = True\n",
        "params.punctuation_probability_threshold = .001\n",
        "params.model_name = \"gpt2\" # change this to \"gpt2-xl\" to get started. \"poetry\" requires a lot more effort to get running-- see the README for details.\n",
        "params.stuck_counter_limit = 1000\n",
        "params.one_syllable_suppression = 20\n",
        "debug = False\n",
        "\n",
        "def xprint(*args, **kwargs):\n",
        "    #only prints if \"debug\" is turned on. It has a try block so that it never throws errors of its own.\n",
        "    global debug\n",
        "    if debug == True:\n",
        "        try:\n",
        "            print(*args, **kwargs)\n",
        "        except:\n",
        "            print(\"error in printing\")\n",
        "                \n",
        "def text_to_meter(text, stress_dictionary):\n",
        "    global punctuation\n",
        "    #calculates the meter of a line of text.\n",
        "    if len(text)==0:\n",
        "        return ''\n",
        "    #capitalize the text\n",
        "    s = text.upper()\n",
        "    #remove any punctuation\n",
        "    whitelist = set('abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
        "    s2 = ''.join(filter(whitelist.__contains__, s))\n",
        "    #split the text into individual words\n",
        "    split_list = re.split('[\\s\\']', s2)\n",
        "    # split_list = list(s2.split(\" \")) \n",
        "    #find the stress for individual words\n",
        "    line_stress=\"\"\n",
        "    for word in split_list:\n",
        "        if len(word)>0:\n",
        "            if word in stress_dictionary:\n",
        "                line_stress = line_stress + stress_dictionary[word]\n",
        "            else:\n",
        "                line_stress = line_stress + \"*\"\n",
        "    return line_stress\n",
        "\n",
        "def rhyme_check(text1,target_rhyme_list,rhyme_dictionary,reverse_rhyme_dictionary,params):\n",
        "    #checks whether text1 and target_rhyme_line rhyme according to a pronunciation dictionary.\n",
        "    global acceptable_punctuation\n",
        "    global bad_rhymes\n",
        "    xprint(\"target_rhyme_list =\")\n",
        "    xprint(target_rhyme_list)\n",
        "    if len(target_rhyme_list)>0:\n",
        "        target_rhyme_line = target_rhyme_list[0]\n",
        "    else:\n",
        "        target_rhyme_line = \"\"\n",
        "    xprint(\"rhyme_check target_rhyme_line =\")\n",
        "    xprint(target_rhyme_line)\n",
        "    #remove punctuation and put both in lower case\n",
        "    text1=text1.strip().lower()\n",
        "    target_rhyme_line=target_rhyme_line.strip().lower()\n",
        "    #token 0, which is \"!\", is code for a line that never will be rhymed with.  \n",
        "    if target_rhyme_line ==\"!\":\n",
        "        return True\n",
        "    #the empty string is code for a line that WILL be rhymed with in the future.\n",
        "    if target_rhyme_line ==\"\":\n",
        "        if text1 ==\"\":\n",
        "            return True\n",
        "        else:\n",
        "            #only words that have enough potential rhymes are allowed.\n",
        "            text1_words = text1.split(\" \")\n",
        "            last_word1 = text1_words[-1]\n",
        "            if last_word1 in rhyme_dictionary:\n",
        "                if rhyme_dictionary[last_word1] in reverse_rhyme_dictionary:\n",
        "                    print(last_word1)\n",
        "                    enough_rhymes =  len(reverse_rhyme_dictionary[rhyme_dictionary[last_word1]])>params.rhyme_set_size\n",
        "                    if enough_rhymes and (not last_word1 in bad_rhymes):         \n",
        "                        return True\n",
        "                    else:\n",
        "                        xprint(\"! not enough rhymes or last word 1 in bad_rhymes\")\n",
        "                        return False\n",
        "                else:\n",
        "                    xprint(\"! not in reverse dictionary \")\n",
        "                    return False      \n",
        "            else:\n",
        "                #the word isn't in the dictionary or there are not enough other words that rhyme with it.\n",
        "                xprint(\"! last word 1 not in rhyme dictionary\")\n",
        "                return False\n",
        "    else:\n",
        "        #the two lines need to actually rhyme\n",
        "        text1_words = text1.split(\" \")\n",
        "        last_word1 = text1_words[-1]\n",
        "        #remove punctuation from the end of the last words\n",
        "        regex = re.compile('[^a-zA-Z]')\n",
        "        last_word1 = regex.sub('', last_word1)\n",
        "        target_rhyme_line_words = target_rhyme_line.split(\" \")\n",
        "        last_word2 = target_rhyme_line_words[-1]\n",
        "        #if tokenizer.encode(last_word2[-1])[0] in acceptable_punctuation:\n",
        "        regex = re.compile('[^a-zA-Z]')\n",
        "        last_word2 = regex.sub('', last_word2)\n",
        "        for line in target_rhyme_list:\n",
        "            target_rhyme_line=line.strip().lower()\n",
        "            target_rhyme_line_words = target_rhyme_line.split(\" \")\n",
        "            last_word2 = target_rhyme_line_words[-1]\n",
        "            regex = re.compile('[^a-zA-Z]')\n",
        "            last_word2 = regex.sub('', last_word2)\n",
        "            print(last_word2)\n",
        "            if last_word1 == last_word2:\n",
        "                #prevent a word rhyming with itself\n",
        "                xprint(\"! a word is rhyming with itself\")\n",
        "                return False\n",
        "        if (last_word1 in rhyme_dictionary) and (last_word2 in rhyme_dictionary):\n",
        "            rhyme1 = rhyme_dictionary[last_word1]\n",
        "            rhyme2 = rhyme_dictionary[last_word2]\n",
        "            rhyme1 = rhyme1.replace(\"0\",\"1\")\n",
        "            rhyme2 = rhyme2.replace(\"0\",\"1\")\n",
        "            #print(rhyme1 + \" vs. \" +  rhyme2)\n",
        "            if (rhyme1 == rhyme2):\n",
        "                return True\n",
        "            else:\n",
        "                xprint(\"! last word1 does not rhyme with last word 2\")\n",
        "                return False\n",
        "        else:\n",
        "            xprint(\"! last word 1 or last word 2 not in rhyme dictionary\")\n",
        "            xprint(last_word1)\n",
        "            xprint(last_word2)\n",
        "            return False\n",
        "       \n",
        "def compare_meters(test_meter,target_meter):\n",
        "    #checks whether test_meter is plausibly matching target_meter. test_meter can include unknown ? stresses. \n",
        "    matchflag=False\n",
        "    \n",
        "    if len(test_meter)>0 and test_meter[-1]==\"*\":\n",
        "        test_meter=test_meter[:-1]\n",
        "    if \"*\" in test_meter[:-1]:\n",
        "        return False\n",
        "    if len(test_meter)<=len(target_meter):\n",
        "        matchflag=True\n",
        "        for character1,character2 in zip(test_meter,target_meter):\n",
        "            if (character1==\"`\" and character2==\"`\") or (character1==\"~\" and character2==\"~\") or character1==\"?\":\n",
        "                pass\n",
        "            else:\n",
        "                matchflag=False\n",
        "    if len(test_meter)==0:\n",
        "        matchflag = True  \n",
        "    #If you want to force it to end on a strongly stressed word, uncomment this.\n",
        "    #elif test_meter[-1] == '?':\n",
        "    #    matchflag = False  \n",
        "    return matchflag\n",
        "\n",
        "def rhyme_and_meter_filter(this_text_sentence,target_rhyme_list,target_meter,probs,params):\n",
        "    #returns a sorted list of words which are (usually) compatible with the upcoming rhyme and meter constraints.\n",
        "    #It's meant to make searching faster, not to be a perfect filter\n",
        "    global stress_tokens\n",
        "    global big_rhymesets\n",
        "    global acceptable_punctuation\n",
        "    global rhyming_tokens\n",
        "    global syllable_tokens\n",
        "    #this randomization helps prevent repetition, but it's kind of a hack.\n",
        "    offset = randint(0,2)\n",
        "    this_meter = text_to_meter(this_text_sentence,stress_dictionary)\n",
        "    xprint(\"target_rhyme_list =\")\n",
        "    xprint(target_rhyme_list)\n",
        "    if len(target_rhyme_list)>0:\n",
        "        target_rhyme = target_rhyme_list[0]\n",
        "    else:\n",
        "        target_rhyme = target_rhyme_list\n",
        "    xprint(\"target_rhyme_list =\")\n",
        "    xprint(target_rhyme_list)\n",
        "    \n",
        "    #meter filter\n",
        "    next_stresses = target_meter[len(this_meter):min(len(this_meter)+3,len(target_meter)+1)]\n",
        "    if len(next_stresses)==0:\n",
        "        return []\n",
        "    all_tokens = set(range(0,50257))\n",
        "    stress_okay = set(stress_tokens[next_stresses])\n",
        "    #all tokens EXCEPT those with okay stress or acceptable punctuation are zeroed out.\n",
        "    for token in all_tokens.difference(stress_okay.union(acceptable_punctuation)):\n",
        "        probs[token] = 0\n",
        "   \n",
        "    #rhyme_filter\n",
        "    xprint(\"meter_length = \", end = \"\")\n",
        "    xprint(len(this_meter))\n",
        "    # lower the probability of one-syllable words, to allow longer words to show up.\n",
        "    too_common_tokens = syllable_tokens[1].union(acceptable_punctuation)\n",
        "    for t in range(0,50257):\n",
        "            if t in too_common_tokens:\n",
        "                probs[t] =probs[t]/params.one_syllable_suppression  \n",
        "    #\"!\" is a code for a non-rhyming sentence. \"\" is code for a sentence that will be rhymed with later.\n",
        "    if len(target_rhyme)>0 and target_rhyme != \"!\":\n",
        "        target_rhyme_words = target_rhyme.split(\" \")\n",
        "        last_target_rhyme_word = target_rhyme_words[-1].strip().lower()     \n",
        "        if last_target_rhyme_word[-1] in {\"!\",\".\",\",\",\";\",\":\",\"?\",\"-\"}:\n",
        "            last_target_rhyme_word = last_target_rhyme_word[:-1]\n",
        "        xprint(\"target rhyme =\",end=\"\")\n",
        "        xprint(last_target_rhyme_word)\n",
        "        these_rhyming_tokens = rhyming_tokens[last_target_rhyme_word]\n",
        "        xprint(tokenizer.decode(these_rhyming_tokens))  \n",
        "        if len(this_meter)==len(target_meter)-1:\n",
        "            for t in range(0,50257):\n",
        "                if t in these_rhyming_tokens:\n",
        "                    pass\n",
        "                else:\n",
        "                    probs[t] = 0\n",
        "        elif len(this_meter)==len(target_meter)-2:\n",
        "            # either a rhyming word or a one-syllable word which could be followed by a rhyming word is okay.\n",
        "            safeset = syllable_tokens[1].union(these_rhyming_tokens)\n",
        "            for t in range(0,50257):\n",
        "                if t in safeset:\n",
        "                    pass\n",
        "                else:\n",
        "                    probs[t] = 0\n",
        "        sorted_probability_list = sorted(enumerate(probs), key=lambda x: x[1], reverse=True)\n",
        "        short_probability_list = sorted_probability_list[0+offset:params.ultimate_expansion+offset]\n",
        "        xprint(\"PART 1\")\n",
        "    elif len(this_meter)>len(target_meter)-3:\n",
        "        sorted_probability_list = sorted(enumerate(probs), key=lambda x: x[1], reverse=True)\n",
        "        short_probability_list = sorted_probability_list[0+offset:params.penultimate_expansion+offset]\n",
        "        xprint(\"PART 2\")\n",
        "    elif len(this_meter)<1:\n",
        "        sorted_probability_list = sorted(enumerate(probs), key=lambda x: x[1], reverse=True)\n",
        "        short_probability_list = sorted_probability_list\n",
        "        xprint(\"PART 3\")\n",
        "    else:\n",
        "        sorted_probability_list = sorted(enumerate(probs), key=lambda x: x[1], reverse=True)\n",
        "        short_probability_list = sorted_probability_list[0+offset:params.other_expansion+offset]\n",
        "        xprint(\"PART 4\")\n",
        "    short_probability_list = [i for i in short_probability_list if i[1] != 0]\n",
        "    xprint(\"short prob list len = \", end =\" \")\n",
        "    xprint(len(short_probability_list))\n",
        "  \n",
        "    return short_probability_list\n",
        "\n",
        "def grow_branches(these_tokens, probs, input_probability,past,params, prompt_length,target_rhyme_list,target_meter):\n",
        "    #recursive function to find all sentence completions\n",
        "    xprint(\"___________________________________________\")\n",
        "    global model\n",
        "    global tokenizer\n",
        "    global stress_dictionary\n",
        "    global rhyme_dictionary\n",
        "    global reverse_rhyme_dictionary\n",
        "    global stuck_counter\n",
        "    global past_backup \n",
        "    stuck_counter = stuck_counter + 1\n",
        "    if stuck_counter > params.stuck_counter_limit:\n",
        "        params.probability_threshold = params.probability_threshold/2\n",
        "        stuck_counter = 0\n",
        "        past = past_backup\n",
        "        these_tokens = these_tokens[:prompt_length]    \n",
        "    found = None\n",
        "    this_text_sentence = tokenizer.decode(these_tokens[prompt_length:])\n",
        "    if len(these_tokens[prompt_length:])<2:\n",
        "        probability_threshold = 0 # no restrictions on the first tokens in each line.\n",
        "    else: \n",
        "        probability_threshold = params.probability_threshold\n",
        "    short_probability_list = rhyme_and_meter_filter(this_text_sentence,target_rhyme_list,target_meter,probs,params)\n",
        "    #proceed only if there are tokens in the probability list that are sufficiently likely to form a sensible sentence.\n",
        "    if len(short_probability_list)==0:\n",
        "        xprint(\"! len(short_probability_list)==0\")\n",
        "        return False\n",
        "    else:\n",
        "        count = 0\n",
        "        #for (this_token,this_probability) in short_probability_list:\n",
        "        #    xprint(tokenizer.decode(this_token), end = \"|\")\n",
        "        for (this_token,this_probability) in short_probability_list:\n",
        "            xprint(\"------------------------------\")\n",
        "            tokens_are_probable_enough_to_continue = this_probability > probability_threshold\n",
        "            xprint(\"tokens are probable: \",end = \"\")\n",
        "            xprint(tokens_are_probable_enough_to_continue)\n",
        "            if not tokens_are_probable_enough_to_continue:\n",
        "                xprint(\"! tokens_are_not probable_enough_to_continue\")\n",
        "                return False\n",
        "            else:\n",
        "                count = count+1\n",
        "                #the token forms the next extension of the current line.\n",
        "                next_probability = this_probability * input_probability\n",
        "                next_tokens = these_tokens.copy()\n",
        "                next_tokens.append(this_token)\n",
        "                next_text_sentence = tokenizer.decode(next_tokens[prompt_length:])\n",
        "                next_meter = text_to_meter(next_text_sentence,stress_dictionary)\n",
        "                xprint(next_meter)\n",
        "                if \"*\" in next_meter[:-1]:\n",
        "                    xprint(\"! * in next meter\")\n",
        "                    return False\n",
        "                meter_check = compare_meters(next_meter,target_meter)\n",
        "                print(next_text_sentence)\n",
        "                if len(next_meter)>len(target_meter):\n",
        "                    xprint(\"! len(next_meter)>len(target_meter)\")\n",
        "                    continue\n",
        "                elif len(next_meter)==len(target_meter):\n",
        "                #this might be a line we want to keep, so we need to verify that the meter and rhyme are good.\n",
        "                    if not meter_check:\n",
        "                        xprint(\"! not meter check\")\n",
        "                        continue\n",
        "                    else:\n",
        "                        rhyme_checks_out = rhyme_check(next_text_sentence,target_rhyme_list,rhyme_dictionary,reverse_rhyme_dictionary,params)\n",
        "                        if not rhyme_checks_out:\n",
        "                            xprint(\"! rhyme doesn't check out\")\n",
        "                            continue\n",
        "                        else:\n",
        "                             # the line has completed successfully\n",
        "                            (word_completion_list,next_past) = expand_node(next_tokens,past)\n",
        "                            sorted_word_completion_list = sorted(enumerate(word_completion_list), key=lambda x: x[1], reverse=True)\n",
        "                            # sometimes it generates the first part of a longer word, that happens to be an actual word. This prevents that from messing things up.\n",
        "                            potential_word_completion = tokenizer.decode(sorted_word_completion_list[0][0])\n",
        "                            all_tokens = set(range(0,50257))\n",
        "                            if potential_word_completion[0] in str.ascii_lowercase: #i.e. it starts with a letter instead of a space, so it's continuing a word\n",
        "                                print(\"! potential_word_completion[0] in str.ascii_lowercase\")\n",
        "                                continue                           \n",
        "                            elif params.line_end_punctuation_constraint == True:\n",
        "                                (end_punctuation_list,next_past) = (word_completion_list,next_past)\n",
        "                                for token in all_tokens.difference(end_punctuation):\n",
        "                                    end_punctuation_list[token] = 0                               \n",
        "                                sorted_end_punctuation_list = sorted(enumerate(end_punctuation_list), key=lambda x: x[1], reverse=True)\n",
        "                                punctuation_probability = sorted_end_punctuation_list [0][1]  \n",
        "                                if punctuation_probability > params.punctuation_probability_threshold:\n",
        "                                    #success!\n",
        "                                    end_punctuation_choice = sorted_end_punctuation_list [0][0] \n",
        "                                    next_tokens.append(end_punctuation_choice)\n",
        "                                    next_text_sentence = tokenizer.decode(next_tokens[prompt_length:])\n",
        "                                    print(\"*** \" + next_text_sentence + \"\\t\" + next_meter)\n",
        "                                    return next_tokens[prompt_length:]\n",
        "                                else:\n",
        "                                    print(\"! end punctuation too rare\")\n",
        "                                    continue\n",
        "                            else:\n",
        "                                #success!\n",
        "                                print(\"*** \" + next_text_sentence + \"\\t\" + next_meter)\n",
        "                                return next_tokens[prompt_length:]  \n",
        "                #If it starts generating strings of punctuation, it rarely recovers. So I put in this hacky check to prevent it.\n",
        "                punctuation_repeats = (len(these_tokens)>1 and these_tokens[-2] in punctuation and these_tokens[-1] in punctuation) or (len(these_tokens)>0 and these_tokens[-1] in punctuation and this_token in punctuation)\n",
        "                line_is_way_too_long = (len(these_tokens[prompt_length+1:])>20)\n",
        "                if next_probability < params.line_probability_threshold or punctuation_repeats or line_is_way_too_long:\n",
        "                    if len(these_tokens[prompt_length+1:])>1:                       \n",
        "                        xprint(\"! len(these_tokens[prompt_length+1:])>1\")\n",
        "                        return False # this returns because if one thing fails the probability check the rest will. Also the repeating punctuation thing needs to be nipped in the bud.\n",
        "                    else:\n",
        "                        xprint(\"! len(these_tokens[prompt_length+1:])<=1\")\n",
        "                        continue #the start of a line gets some leeway so that it doesn't return false and abandon the whole poem.\n",
        "                else:\n",
        "                    found = False\n",
        "                    if meter_check and len(next_meter)<len(target_meter):\n",
        "                        #this isn't long enough to be a complete line, but it could be the start of a complete line, so we expand it.\n",
        "                        (next_probability_list,next_past) = expand_node(next_tokens,past)\n",
        "                        found = grow_branches(next_tokens,next_probability_list, next_probability, next_past,params,prompt_length,target_rhyme_list,target_meter)\n",
        "                    if found != False:\n",
        "                        xprint(\"found =\", end =\"\")\n",
        "                        xprint(found)\n",
        "                        return found\n",
        "    xprint(\"! end of function\")\n",
        "    return False\n",
        "\n",
        "def expand_node(sentence, past):\n",
        "    #finds probabilities for the next token using gpt-2. This is the only computationally expensive operation in the program.\n",
        "    global model\n",
        "    if past == None:\n",
        "        input_ids = torch.tensor(sentence).unsqueeze(0)\n",
        "    else:\n",
        "        input_ids = torch.tensor([sentence[-1]]).unsqueeze(0)\n",
        "    inputs = {'input_ids': input_ids}    \n",
        "    with torch.no_grad():\n",
        "        logits, past = model(**inputs, past=past)\n",
        "        logits[0][0][50256]=-math.inf # no <end of text> token\n",
        "        logits = logits[:, -1, :]  \n",
        "        probs = F.softmax(logits, dim=-1).tolist()[0]\n",
        "        return (probs, past)\n",
        "\n",
        "def create_stress_dictionary():\n",
        "    pronounce_file = open(\"pronounce.txt\", \"r\")\n",
        "    stress_dictionary = {}\n",
        "    for line in pronounce_file:\n",
        "        line = line.strip(\"\\n\")\n",
        "        parts = line.split(\" \")\n",
        "        syllable_list = parts[2:]\n",
        "        word = parts[0]\n",
        "        stresses=\"\"\n",
        "        if word in [\"A\",\"AN\",\"THE\",\"AND\",\"BUT\",\"OR\"]:\n",
        "            stresses=\"~\"\n",
        "        elif word in [\"I\",\"YOU\",\"HE\",\"SHE\",\"IT\",\"WE\",\"THEY\",\"MY\",\"HIS\",\"HER\",\"ITS\",\"OUR\",\"YOUR\",\"THEIR\",\"OURS\",\"YOURS\",\"THEIRS\",\"AM\",\"IS\",\"ARE\",\"WAS\",\"WERE\",\"BEEN\",\"BE\",\"HAVE\",\"HAS\",\"HAD\",\"DO\",\"DOES\",\"DID\",\"WILL\",\"WOULD\",\"SHALL\",\"SHOULD\",\"MAY\",\"MIGHT\",\"MUST\",\"CAN\",\"COULD\",\"OF\",\"WITH\",\"AT\",\"FROM\",\"TO\",\"IN\",\"FOR\",\"ON\",\"BY\",\"LIKE\",\"SINCE\",\"UP\",\"OFF\",\"NEAR\",\"WHICH\",\"AS\",\"EACH\",\"SO\",\"THAT\",\"THATS\"]:\n",
        "            stresses=\"?\"    \n",
        "        else:\n",
        "            for syllable in syllable_list:\n",
        "                if syllable.endswith(\"1\"):\n",
        "                    stresses=stresses+\"`\"\n",
        "                elif syllable.endswith(\"0\"):\n",
        "                    stresses=stresses+\"~\"\n",
        "                elif syllable.endswith(\"2\"):\n",
        "                    stresses=stresses+\"?\"\n",
        "        if word in {\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"}:\n",
        "            pass\n",
        "        else:\n",
        "            stress_dictionary[word] = stresses\n",
        "    return stress_dictionary\n",
        "\n",
        "def create_rhyme_dictionary(tokenizer):\n",
        "    pronounce_file = open(\"pronounce.txt\", \"r\")\n",
        "    rhyme_dictionary = {}\n",
        "    reverse_rhyme_dictionary = {}\n",
        "    syllable_count_dictionary = {}\n",
        "    for line in pronounce_file:\n",
        "        line = line.strip()\n",
        "        if line.startswith(';'): continue\n",
        "        word, phones = line.split(\"  \")\n",
        "        syllables = phones.split(\" \")\n",
        "        syllable_count_dictionary[word]=phones.count(\"0\")+phones.count(\"1\")+phones.count(\"2\")\n",
        "        join_flag = 0\n",
        "        outstring = ''\n",
        "        for syllable in syllables:\n",
        "            if join_flag == 0:\n",
        "                if \"1\" in syllable:\n",
        "                    join_flag = 1\n",
        "                    outstring = syllable\n",
        "            else:\n",
        "                outstring = outstring + \" \" + syllable\n",
        "        if outstring == \"\":\n",
        "            for syllable in syllables:\n",
        "                if join_flag == 0:\n",
        "                    if \"0\" in syllable:\n",
        "                        join_flag = 1\n",
        "                        outstring = syllable\n",
        "                else:\n",
        "                    outstring = outstring + \" \" + syllable\n",
        "        rhyme_dictionary[word.lower()] = outstring\n",
        "        if outstring in reverse_rhyme_dictionary:\n",
        "            reverse_rhyme_dictionary[outstring].append(word.lower())\n",
        "        else:\n",
        "            reverse_rhyme_dictionary[outstring]=[word.lower()]\n",
        "    \n",
        "    rhyming_tokens = pickle.load( open( \"rhyming_tokens.p\", \"rb\" ) )\n",
        "    syllable_tokens = pickle.load( open( \"syllable_tokens.p\", \"rb\" ) )\n",
        "    \n",
        "    bad_rhymes = [\"a\",\"an\",\"it\",\"is\",\"as\",\"at\",\"was\",\"of\",\"at\",\"that\",\n",
        "                     \"has\",\"your\",\"my\",\"his\",\"their\",\"on\",\"for\",\"its\",\"to\",\n",
        "                     \"from\",\"if\",\"ur\",\"re\",\"our\",\"un\",\"dis\",\"diss\",\"mis\",\n",
        "                     \"wat\",\"com\",\"comm\",\"psych\",\"lol\",\"vis\",\"al\",\"los\",\"pol\",\n",
        "                     \"bis\",\"up\", \" la\",\"sa\",\"ha\",\"mah\", \" wal\", \"lat\", \"ot\", \"sol\",\n",
        "                     \"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\n",
        "                     \"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n",
        "    return rhyme_dictionary, reverse_rhyme_dictionary, bad_rhymes, syllable_count_dictionary, rhyming_tokens, syllable_tokens\n",
        "\n",
        "def poem_scheme(kind):\n",
        "    #in \"rhyme_scheme\" the first thing in the list for each line must be the line you want to rhyme with. \n",
        "    # After that you can list other lines you want to avoid repeating the last word.\n",
        "    # \"meter_scheme\" is ` for accented, ~ for unaccented syllable. \n",
        "    global poem_line\n",
        "    if kind == \"limerick\":\n",
        "        number_of_lines = 5\n",
        "        meter_scheme = [\"\"] * number_of_lines\n",
        "        for line in {0,1,4}:\n",
        "            meter_scheme[line] = \"~`~~`~~`\" \n",
        "        for line in {2,3}:\n",
        "            meter_scheme[line] = \"~`~~`\"\n",
        "        #meter_scheme[0] = \"~`\" # if you want to start with a prompt like \"There once was a girl from\"\n",
        "        rhyme_scheme = [ \"\", [poem_line[0]], \"\", [poem_line[2]], [poem_line[0], poem_line[1]] ]\n",
        "    if kind == \"sonnet\":\n",
        "        number_of_lines = 10\n",
        "        meter_scheme = [\"\"] * number_of_lines\n",
        "        for line in range(0,number_of_lines):\n",
        "            meter_scheme[line] = \"~`~`~`~`~`\"\n",
        "        rhyme_scheme = [\"\",\"\",[poem_line[0]],[poem_line[1]],\"\",\"\",[poem_line[4]],[poem_line[5]],\"\",[poem_line[8]] ]\n",
        "    if kind == \"blank verse\":\n",
        "        number_of_lines = 10\n",
        "        meter_scheme = [\"\"] * number_of_lines\n",
        "        for line in range(0,number_of_lines):\n",
        "            meter_scheme[line] = \"~`~`~`~`~`\"\n",
        "        rhyme_scheme = [[0]]*number_of_lines\n",
        "    if kind == \"couplets\":\n",
        "        number_of_lines = 10\n",
        "        meter_scheme = [\"\"] * number_of_lines\n",
        "        for line in range(0,number_of_lines):\n",
        "            meter_scheme[line] = \"`~`~`~\"\n",
        "        rhyme_scheme = [\"\",[poem_line[0]],\"\",[poem_line[2]],\"\",[poem_line[4]],\"\",[poem_line[6]],\"\",[poem_line[8]] ]\n",
        "    if kind == \"mini-couplets\":\n",
        "        number_of_lines = 20\n",
        "        meter_scheme = [\"\"] * number_of_lines\n",
        "        for line in range(0,number_of_lines):\n",
        "            meter_scheme[line] = \"~`~`\"\n",
        "        rhyme_scheme = [\"\",[poem_line[0]],\"\",[poem_line[2]],\"\",[poem_line[4]],\"\",[poem_line[6]],\"\",[poem_line[8]],\"\",[poem_line[10]],\"\",[poem_line[12]],\"\",[poem_line[14]],\"\",[poem_line[16]],\"\",[poem_line[18]] ]\n",
        "        params.penultimate_expansion = 10000\n",
        "    if kind == \"ballad\":\n",
        "        number_of_lines = 16\n",
        "        meter_scheme = [\"\"] * number_of_lines\n",
        "        for line in {0,2,4,6,8,10,12,14}:\n",
        "            meter_scheme[line] = \"~`~`~`~`\"\n",
        "        for line in {1,3,5,7,9,11,13,15}:\n",
        "            meter_scheme[line] = \"~`~`~`\"\n",
        "        rhyme_scheme = [[0],\"\",[0],[poem_line[1]],[0],\"\",[0],[poem_line[5]],[0],\"\",[0],[poem_line[9]],[0],\"\",[0],[poem_line[13]]]\n",
        "    return number_of_lines, rhyme_scheme, meter_scheme\n",
        "            \n",
        "#-----------------------------------------------\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') \n",
        "rhyme_dictionary, reverse_rhyme_dictionary, bad_rhymes, syllable_count_dictionary, rhyming_tokens, syllable_tokens = create_rhyme_dictionary(tokenizer)\n",
        "stress_dictionary = create_stress_dictionary()   \n",
        "stress_tokens = pickle.load( open(\"stress_tokens.p\", \"rb\"))\n",
        "xprint(\"rhymes loaded\")\n",
        "#load gpt-2 (takes a few seconds)                \n",
        "model = GPT2LMHeadModel.from_pretrained(params.model_name)\n",
        "xprint(\"model loaded\")\n",
        "#-----------------------------------------------\n",
        "#from here on must be run every time you want to create a new poem. If you want to generate multiple poems, maybe wrap this in a while-loop?\n",
        "seed(params.random_seed)\n",
        "with torch.no_grad():\n",
        "    raw_prompt = input(\"starting prompt: \")\n",
        "    prompt = tokenizer.encode(raw_prompt)\n",
        "    original_length = len(prompt)\n",
        "    past = None\n",
        "    (probs, past) = expand_node(prompt, None) \n",
        "    scheme = input(\"ballad, limerick, couplets or sonnet? \")\n",
        "    poem_line = [\"\"] * 100000 #this just has to be long enough the next line will never complain-- fixed two lines down\n",
        "    number_of_lines, rhyme_scheme, meter_scheme = poem_scheme(scheme)\n",
        "    poem_line = [\"\"] * number_of_lines  \n",
        "    line = 0\n",
        "    backup_prompts = [\"\"]*100\n",
        "    backup_pasts = [\"\"]*100\n",
        "    while line < number_of_lines:\n",
        "        stuck_counter = 0\n",
        "        backup_prompts[line]=prompt\n",
        "        backup_pasts[line]=past\n",
        "        number_of_lines, rhyme_scheme, meter_scheme = poem_scheme(scheme)\n",
        "        target_rhyme_list = []\n",
        "        for target_rhyme_line in rhyme_scheme[line]:\n",
        "            target_rhyme_list.append(tokenizer.decode(target_rhyme_line))\n",
        "        if target_rhyme_list == []:\n",
        "            target_rhyme_list = \"\"\n",
        "        xprint(target_rhyme_list)\n",
        "        target_meter = meter_scheme[line]\n",
        "        this_line = grow_branches(prompt,probs,1,past,params,len(prompt),target_rhyme_list,target_meter)\n",
        "        if this_line == False:\n",
        "            print(\"something went wrong, quitting\")\n",
        "            break\n",
        "        poem_line[line] = this_line\n",
        "        line = line + 1\n",
        "        prompt = prompt  + this_line\n",
        "        past_backup = past\n",
        "        (probs, past) = expand_node(prompt, None) \n",
        "    # this just changes the last token to a period. Very hacky.\n",
        "    if poem_line[-1][-1] in end_punctuation:\n",
        "        poem_line[-1][-1] = tokenizer.encode('.')[0]\n",
        "    print()\n",
        "    print(tokenizer.decode(prompt[original_length:]))\n",
        "    print()\n",
        "    print(tokenizer.decode(poem_line[0]))\n",
        "    for line in range(1,number_of_lines):\n",
        "        if poem_line[line][0] in acceptable_punctuation:\n",
        "            poem_line[line-1].append(poem_line[line][0])\n",
        "            poem_line[line] = poem_line[line][1:]\n",
        "    for line in range(1,number_of_lines):\n",
        "        print(tokenizer.decode(poem_line[line]))\n",
        "        \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d7e1f92ab4c43d4bb7b0ae710830ca8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2dadeba3edc541e1946d68934d340096",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84cc8cbe1b914d24a94e1c1c0c3cde2f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29dfe44d911e4c05b3c230b06b2f764c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "starting prompt: hi\n",
            "ballad, limerick, couplets or sonnet? sonnet\n",
            ",\n",
            ", who\n",
            ", the\n",
            ", the former\n",
            ", the former leader\n",
            ", the former leader and\n",
            ", the former leader in\n",
            ", the former leader in Iran\n",
            ", the former leader in Iran,\n",
            ", the former leader in Iran, has\n",
            ", the former leader in Iran, has been\n",
            "been\n",
            "! end punctuation too rare\n",
            ", the former leader in Iran, has said\n",
            "said\n",
            "*** , the former leader in Iran, has said.\t~`~`~??`?`\n",
            " Iran\n",
            " Iran has\n",
            " Iran has threatened\n",
            " Iran has threatened to\n",
            " Iran has threatened to retaliate\n",
            "retaliate\n",
            " Iran has threatened to attack\n",
            " Iran has threatened to attack Iran\n",
            "iran\n",
            "***  Iran has threatened to attack Iran,\t?`?`~?~`?`\n",
            " accusing\n",
            " accusing it\n",
            " accusing it of\n",
            " accusing it of using\n",
            " accusing it of using weapons\n",
            " accusing it of using missiles\n",
            " accusing it of using cyber\n",
            " accusing it of using its\n",
            " accusing it of using its inter\n",
            "said\n",
            " accusing it of using its new\n",
            " accusing it of using missile\n",
            " accusing it of using proxies\n",
            " accusing it of using the\n",
            " accusing it of using human\n",
            " accusing it of using poison\n",
            " accusing it of using anti\n",
            " accusing it of using a\n",
            " accusing it of using \"\n",
            " accusing it of using covert\n",
            " accusing it of using terror\n",
            " accusing it of using proxy\n",
            " accusing it of using rocket\n",
            " accusing it of using rockets\n",
            " accusing it of using heavy\n",
            " accusing it of using other\n",
            " accusing it of using inter\n",
            " accusing it of using any\n",
            " accusing it of using Western\n",
            " accusing it of using deadly\n",
            " accusing it of using tunnels\n",
            " accusing it of using peaceful\n",
            " accusing it of using Shiite\n",
            " accusing it of using cluster\n",
            " accusing it of using sanctions\n",
            " accusing it of using torture\n",
            " accusing it of using natural\n",
            " accusing it of using an\n",
            " accusing it of using aircraft\n",
            " accusing it of using foreign\n",
            " accusing it of using foreign n\n",
            "said\n",
            " accusing it of using highly\n",
            " accusing it of using hundreds\n",
            " accusing it of using thousands\n",
            " accusing it of using water\n",
            " accusing it of using mustard\n",
            " accusing it of using chlorine\n",
            " accusing it of using secret\n",
            " accusing it of using money\n",
            " accusing it of using blackmail\n",
            " accusing it of using pressure\n",
            " accusing it of using stolen\n",
            " accusing it of using women\n",
            " accusing it of using hydro\n",
            " accusing it of using special\n",
            " accusing it of using modern\n",
            " accusing it of using Shia\n",
            " accusing it of using people\n",
            " accusing it of using ther\n",
            " accusing it of using forces\n",
            " accusing it of using Israel\n",
            " accusing it of using massive\n",
            " accusing it of using certain\n",
            " accusing it of using social\n",
            " accusing it of using many\n",
            " accusing it of using tunnel\n",
            " accusing it of using leverage\n",
            " accusing it of using force\n",
            " accusing it of using force to\n",
            " accusing it of using force in\n",
            " accusing it of using force.\n",
            " accusing it of using force. The\n",
            " accusing it of using force. He\n",
            " accusing it of using force. It\n",
            " accusing it of using force. But\n",
            " accusing it of using force. In\n",
            " accusing it of using force. A\n",
            " accusing it of using force. On\n",
            " accusing it of using force. U\n",
            " accusing it of using force. And\n",
            " accusing it of using force. This\n",
            " accusing it of using force. That\n",
            " accusing it of using force. That said\n",
            "said\n",
            " accusing it of using force. His\n",
            " accusing it of using force. At\n",
            " accusing it of using force. As\n",
            " accusing it of using force. As she\n",
            "said\n",
            " accusing it of using force. An\n",
            " accusing it of using force. An un\n",
            "said\n",
            " accusing it of using force. There\n",
            " accusing it of using force. She\n",
            " accusing it of using force. If\n",
            " accusing it of using force. North\n",
            " accusing it of using force. Since\n",
            " accusing it of using force. Since she\n",
            "said\n",
            " accusing it of using force. They\n",
            " accusing it of using force. One\n",
            " accusing it of using force. Its\n",
            " accusing it of using force and\n",
            " accusing it of using force on\n",
            " accusing it of using force if\n",
            " accusing it of using force instead\n",
            "said\n",
            "***  accusing it of using force instead.\t~`~??`~`?`\n",
            " Iranian\n",
            " Israel\n",
            " Obama\n",
            " Obama has\n",
            " Obama has refused\n",
            " Obama has refused,\n",
            " Obama has refused, insisting\n",
            " Obama has refused, insisting a\n",
            "iran\n",
            " Obama has refused, insisting at\n",
            "iran\n",
            " Obama has refused, insisting an\n",
            " Obama has refused, however\n",
            " Obama has refused, despite\n",
            " Obama has refused, despite the\n",
            " Obama has refused, despite a\n",
            " Obama has refused, despite his\n",
            " Obama has refused, despite U\n",
            " Obama has refused, despite U.\n",
            " Obama has refused, despite an\n",
            " Obama has refused, despite this\n",
            " Obama has refused, despite its\n",
            " Obama has refused, despite its den\n",
            "iran\n",
            " Obama has refused, despite den\n",
            " Obama has refused, despite that\n",
            " Obama has refused, despite that there\n",
            "iran\n",
            " Obama has refused, despite that a\n",
            "iran\n",
            " Obama has refused, despite that an\n",
            " Obama has refused, despite that den\n",
            "iran\n",
            " Obama has refused, despite all\n",
            " Obama has refused, despite it\n",
            " Obama has refused, despite their\n",
            " Obama has refused, despite high\n",
            " Obama has refused, despite such\n",
            " Obama has refused, despite new\n",
            " Obama has refused, instead\n",
            " Obama has refused, instead def\n",
            " Obama has refused, instead he\n",
            " Obama has refused, instead to\n",
            " Obama has refused, instead to go\n",
            "iran\n",
            " Obama has refused, instead to do\n",
            "iran\n",
            " Obama has refused, instead to step\n",
            "iran\n",
            " Obama has refused, instead in\n",
            " Obama has refused, instead on\n",
            " Obama has refused, accusing\n",
            " Obama has refused, even\n",
            " Obama has refused, and\n",
            " Obama has refused, and even\n",
            " Obama has refused, and even at\n",
            "iran\n",
            " Obama has refused, and even a\n",
            "iran\n",
            " Obama has refused, and even won\n",
            "iran\n",
            " Obama has refused, and many\n",
            " Obama has refused, and many at\n",
            "iran\n",
            " Obama has refused, and many do\n",
            "iran\n",
            " Obama has refused, and the\n",
            " Obama has refused, and has\n",
            " Obama has refused, and has so\n",
            " Obama has refused, and has sent\n",
            " Obama has refused, and has since\n",
            " Obama has refused, and has had\n",
            " Obama has refused, and has had a\n",
            "iran\n",
            " Obama has refused, and has had an\n",
            " Obama has refused, and has had at\n",
            "iran\n",
            " Obama has refused, and has def\n",
            " Obama has refused, and has no\n",
            " Obama has refused, and has left\n",
            " Obama has refused, and has in\n",
            " Obama has refused, and has in an\n",
            " Obama has refused, and has in so\n",
            "iran\n",
            " Obama has refused, and has come\n",
            " Obama has refused, and has long\n",
            " Obama has refused, and has cast\n",
            " Obama has refused, and has led\n",
            " Obama has refused, and has a\n",
            " Obama has refused, and Congress\n",
            " Obama has refused, and Congress won\n",
            "iran\n",
            " Obama has refused, and Russia\n",
            " Obama has refused, and Russia has\n",
            "iran\n",
            " Obama has refused, and Russia on\n",
            "iran\n",
            "! end punctuation too rare\n",
            " Obama has refused, and Israel\n",
            " Obama has refused, and other\n",
            " Obama has refused, and only\n",
            " Obama has refused, and only on\n",
            "iran\n",
            "! end punctuation too rare\n",
            " Obama has refused, and only a\n",
            "iran\n",
            " Obama has refused, and only has\n",
            "iran\n",
            " Obama has refused, and only at\n",
            "iran\n",
            " Obama has refused, and only so\n",
            "iran\n",
            " Obama has refused, and only an\n",
            " Obama has refused, and only won\n",
            "iran\n",
            " Obama has refused, and Kerry\n",
            " Obama has refused, and Kerry has\n",
            "iran\n",
            " Obama has refused, and Kerry on\n",
            "iran\n",
            "! end punctuation too rare\n",
            " Obama has refused, and also\n",
            " Obama has refused, and also won\n",
            "iran\n",
            " Obama has refused, and also a\n",
            "iran\n",
            " Obama has refused, and also at\n",
            "iran\n",
            " Obama has refused, and sanctions\n",
            " Obama has refused, and sanctions won\n",
            "iran\n",
            " Obama has refused, and Saudi\n",
            " Obama has refused, and threatened\n",
            " Obama has refused, and threatened on\n",
            "iran\n",
            "! end punctuation too rare\n",
            " Obama has refused, and threatened at\n",
            "iran\n",
            " Obama has refused, and hasn\n",
            " Obama has refused, and others\n",
            " Obama has refused, and others do\n",
            "iran\n",
            " Obama has refused, and others has\n",
            "iran\n",
            " Obama has refused, and he\n",
            " Obama has refused, and he is\n",
            " Obama has refused, and he is on\n",
            "iran\n",
            "! end punctuation too rare\n",
            " Obama has refused, and he is at\n",
            "iran\n",
            " Obama has refused, and he is a\n",
            "iran\n",
            " Obama has refused, and he will\n",
            " Obama has refused, and he will do\n",
            "iran\n",
            " Obama has refused, and he will go\n",
            "iran\n",
            " Obama has refused, and he will step\n",
            "iran\n",
            " Obama has refused, and he will at\n",
            "iran\n",
            " Obama has refused, and he was\n",
            " Obama has refused, and he was a\n",
            "iran\n",
            " Obama has refused, and he was on\n",
            "iran\n",
            "! end punctuation too rare\n",
            " Obama has refused, and he was at\n",
            "iran\n",
            " Obama has refused, and he was so\n",
            "iran\n",
            " Obama has refused, and he was an\n",
            " Obama has refused, and he and\n",
            " Obama has refused, and he and a\n",
            "iran\n",
            " Obama has refused, and he did\n",
            " Obama has refused, and he did a\n",
            "iran\n",
            " Obama has refused, and he did at\n",
            "iran\n",
            " Obama has refused, and he may\n",
            " Obama has refused, and he may do\n",
            "iran\n",
            " Obama has refused, and he may go\n",
            "iran\n",
            " Obama has refused, and he may or\n",
            " Obama has refused, and he may step\n",
            "iran\n",
            " Obama has refused, and he may at\n",
            "iran\n",
            " Obama has refused, and he could\n",
            " Obama has refused, and he could do\n",
            "iran\n",
            " Obama has refused, and he could step\n",
            "iran\n",
            " Obama has refused, and he could at\n",
            "iran\n",
            " Obama has refused, and he had\n",
            " Obama has refused, and he had on\n",
            "iran\n",
            "! end punctuation too rare\n",
            " Obama has refused, and he would\n",
            " Obama has refused, and he would go\n",
            "iran\n",
            " Obama has refused, and he would step\n",
            "iran\n",
            " Obama has refused, and he does\n",
            " Obama has refused, and he should\n",
            " Obama has refused, and he should step\n",
            "iran\n",
            " Obama has refused, and he should go\n",
            "iran\n",
            " Obama has refused, and he should at\n",
            "iran\n",
            " Obama has refused, and he can\n",
            " Obama has refused, and he can at\n",
            "iran\n",
            " Obama has refused, and he def\n",
            " Obama has refused, and he must\n",
            " Obama has refused, and he must step\n",
            "iran\n",
            " Obama has refused, and he must go\n",
            "iran\n",
            " Obama has refused, and he must at\n",
            "iran\n",
            " Obama has refused, and he might\n",
            " Obama has refused, and he might go\n",
            "iran\n",
            " Obama has refused, and he might step\n",
            "iran\n",
            " Obama has refused, and he might at\n",
            "iran\n",
            " Obama has refused, and he might.\n",
            " Obama has refused, and he sent\n",
            " Obama has refused, and neither\n",
            " Obama has refused, and neither do\n",
            "iran\n",
            " Obama has refused, and neither an\n",
            " Obama has refused, and his\n",
            " Obama has refused, and his new\n",
            " Obama has refused, and his first\n",
            " Obama has refused, and his U\n",
            " Obama has refused, and his call\n",
            " Obama has refused, and his main\n",
            " Obama has refused, and his two\n",
            " Obama has refused, and his most\n",
            " Obama has refused, and his def\n",
            " Obama has refused, and China\n",
            " Obama has refused, and Mr\n",
            " Obama has refused, and Mr de\n",
            "iran\n",
            " Obama has refused, and Moscow\n",
            " Obama has refused, and after\n",
            " Obama has refused, and Putin\n",
            " Obama has refused, and promised\n",
            " Obama has refused, and promised an\n",
            " Obama has refused, and promised on\n",
            "iran\n",
            "! end punctuation too rare\n",
            " Obama has refused, and promised at\n",
            "iran\n",
            " Obama has refused, and promised there\n",
            "iran\n",
            " Obama has refused, and is\n",
            " Obama has refused, and is at\n",
            " Obama has refused, and is at a\n",
            "iran\n",
            " Obama has refused, and is at an\n",
            " Obama has refused, and is no\n",
            " Obama has refused, and is to\n",
            " Obama has refused, and is to do\n",
            "iran\n",
            " Obama has refused, and is to step\n",
            "iran\n",
            " Obama has refused, and is far\n",
            " Obama has refused, and is back\n",
            " Obama has refused, and is a\n",
            " Obama has refused, and is the\n",
            " Obama has refused, and is out\n",
            " Obama has refused, and in\n",
            " Obama has refused, and in a\n",
            " Obama has refused, and in his\n",
            " Obama has refused, and in an\n",
            " Obama has refused, and in fact\n",
            " Obama has refused, and in March\n",
            " Obama has refused, and in May\n",
            " Obama has refused, and in May a\n",
            "iran\n",
            " Obama has refused, and in May has\n",
            "iran\n",
            " Obama has refused, and in May on\n",
            "iran\n",
            "! end punctuation too rare\n",
            " Obama has refused, and in May an\n",
            " Obama has refused, and in May at\n",
            "iran\n",
            " Obama has refused, and in May there\n",
            "iran\n",
            " Obama has refused, and in this\n",
            " Obama has refused, and in one\n",
            " Obama has refused, and in mid\n",
            " Obama has refused, and in that\n",
            " Obama has refused, and in its\n",
            " Obama has refused, and in New\n",
            " Obama has refused, and in two\n",
            " Obama has refused, and in all\n",
            " Obama has refused, and in most\n",
            " Obama has refused, and in part\n",
            " Obama has refused, and it\n",
            " Obama has refused, and it is\n",
            " Obama has refused, and it is an\n",
            " Obama has refused, and it is on\n",
            "iran\n",
            "***  Obama has refused, and it is on,\t?`~?~`~???\n",
            " record\n",
            " record that\n",
            " record that Israel\n",
            " record that he\n",
            " record that he has\n",
            " record that he has threatened\n",
            " record that he has threatened action\n",
            " record that he has threatened action.\n",
            " record that he has threatened action. The\n",
            " record that he has threatened action. He\n",
            "he\n",
            "***  record that he has threatened action. He,\t~`???`~`~?\n",
            " along\n",
            " along with\n",
            " along with others\n",
            " along with others on\n",
            " along with others on Obama\n",
            " along with others on Obama-\n",
            " along with others on Obama and\n",
            " along with others on Obama.\n",
            " along with others on Obama. And\n",
            " along with others on Obama. The\n",
            " along with others on Iran\n",
            " along with others on Iran,\n",
            " along with others on Iran, have\n",
            " along with others on Iran, have said\n",
            "said\n",
            "***  along with others on Iran, have said,\t~`?`~??`?`\n",
            " \"\n",
            " without\n",
            " without elabor\n",
            " without providing\n",
            " without providing concrete\n",
            " without providing further\n",
            " without providing further detail\n",
            " without providing further comment\n",
            " without providing further comment and\n",
            " without providing further comment at\n",
            "he\n",
            " without providing further comment for\n",
            "he\n",
            " without providing further context\n",
            " without providing further context and\n",
            " without providing further context for\n",
            "he\n",
            " without providing further background\n",
            " without providing further background and\n",
            " without providing further background for\n",
            "he\n",
            " without providing further background as\n",
            "he\n",
            " without providing further data\n",
            " without providing further data and\n",
            " without providing further data for\n",
            "he\n",
            " without providing further comments\n",
            " without providing further comments and\n",
            " without providing further comments at\n",
            "he\n",
            " without providing further comments for\n",
            "he\n",
            " without providing further figures\n",
            " without providing further figures.\n",
            " without providing further figures and\n",
            " without providing further figures for\n",
            "he\n",
            " without providing further action\n",
            " without providing further action and\n",
            " without providing further action for\n",
            "he\n",
            " without providing further action at\n",
            "he\n",
            " without providing further action as\n",
            "he\n",
            " without providing further proof\n",
            " without providing further proof that\n",
            " without providing further proof that we\n",
            "he\n",
            "***  without providing further proof that we,\t~`~`~`~`??\n",
            " Iran\n",
            " Iran,\n",
            " Iran, have\n",
            " Iran, have ever\n",
            " Iran, have ever threatened\n",
            " Iran, have ever threatened any\n",
            " Iran, have ever threatened to\n",
            " Iran, have ever threatened to do\n",
            " Iran, have ever threatened to bomb\n",
            " Iran, have ever threatened to go\n",
            " Iran, have ever threatened to inter\n",
            "said\n",
            " Iran, have ever threatened to be\n",
            " Iran, have ever threatened action\n",
            " Iran, have ever threatened action we\n",
            "said\n",
            " Iran, have ever threatened Israel\n",
            " Iran, have ever threatened such\n",
            " Iran, have ever threatened such war\n",
            " Iran, have ever threatened such.\n",
            " Iran, have ever threatened such. We\n",
            " Iran, have ever threatened such. And\n",
            " Iran, have ever threatened such. The\n",
            " Iran, have ever threatened such. I\n",
            " Iran, have ever threatened such. I said\n",
            "said\n",
            " Iran, have ever threatened such. I read\n",
            "said\n",
            "***  Iran, have ever threatened such. I read,\t?`?`~`~`?`\n",
            " however\n",
            " however strongly\n",
            " however strongly I\n",
            " however strongly I support\n",
            " however strongly I support this\n",
            " however strongly I support,\n",
            " however strongly I support, that\n",
            " however strongly I support, that the\n",
            " however strongly I support, that there\n",
            "there\n",
            "***  however strongly I support, that there,\t?`~`~?~`?`\n",
            " along\n",
            " along the\n",
            " along the lines\n",
            " along the lines suggested\n",
            " along the lines suggested by\n",
            " along the lines suggested by this\n",
            " along the lines suggested by our\n",
            " along the lines suggested by a\n",
            " along the lines suggested by my\n",
            " along the lines suggested by your\n",
            " along the lines suggested by you\n",
            " along the lines suggested by you there\n",
            "there\n",
            " along the lines suggested by you for\n",
            "there\n",
            " along the lines suggested by you all\n",
            "there\n",
            " along the lines suggested by you a\n",
            "there\n",
            " along the lines suggested by you we\n",
            "there\n",
            " along the lines suggested by his\n",
            " along the lines suggested by that\n",
            " along the lines suggested by one\n",
            " along the lines suggested by an\n",
            " along the lines suggested by an all\n",
            "there\n",
            " along the lines suggested by an ad\n",
            "there\n",
            " along the lines suggested by U\n",
            " along the lines suggested by him\n",
            " along the lines suggested by Ay\n",
            " along the lines suggested by P\n",
            " along the lines suggested by Sen\n",
            " along the lines suggested by Al\n",
            " along the lines suggested by all\n",
            " along the lines suggested by R\n",
            " along the lines suggested by The\n",
            " along the lines suggested by H\n",
            " along the lines suggested by N\n",
            " along the lines suggested by N'\n",
            " along the lines suggested by M\n",
            " along the lines suggested by M'\n",
            " along the lines suggested by most\n",
            " along the lines suggested by I\n",
            " along the lines suggested by I can\n",
            "there\n",
            " along the lines suggested by I for\n",
            "there\n",
            " along the lines suggested by J\n",
            " along the lines suggested by T\n",
            " along the lines suggested by Rep\n",
            " along the lines suggested by D\n",
            " along the lines suggested by D-\n",
            " along the lines suggested by D'\n",
            " along the lines suggested by G\n",
            " along the lines suggested by Gen\n",
            " along the lines suggested by such\n",
            " along the lines suggested by S\n",
            " along the lines suggested by me\n",
            " along the lines suggested by their\n",
            " along the lines suggested by al\n",
            " along the lines suggested by W\n",
            " along the lines suggested by Bill\n",
            " along the lines suggested by Ha\n",
            " along the lines suggested by it\n",
            " along the lines suggested by it-\n",
            " along the lines suggested by it- there\n",
            "there\n",
            " along the lines suggested by it- we\n",
            "there\n",
            " along the lines suggested by it- a\n",
            "there\n",
            " along the lines suggested by it- they\n",
            "there\n",
            " along the lines suggested by it- where\n",
            "there\n",
            "***  along the lines suggested by it- where,\t~`~`~`~??`\n",
            "\n",
            ", the former leader in Iran, has said. Iran has threatened to attack Iran, accusing it of using force instead. Obama has refused, and it is on, record that he has threatened action. He, along with others on Iran, have said, without providing further proof that we, Iran, have ever threatened such. I read, however strongly I support, that there, along the lines suggested by it- where,\n",
            "\n",
            ", the former leader in Iran, has said.\n",
            " Iran has threatened to attack Iran,\n",
            " accusing it of using force instead.\n",
            " Obama has refused, and it is on,\n",
            " record that he has threatened action. He,\n",
            " along with others on Iran, have said,\n",
            " without providing further proof that we,\n",
            " Iran, have ever threatened such. I read,\n",
            " however strongly I support, that there,\n",
            " along the lines suggested by it- where.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8WaqCwMkczI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}