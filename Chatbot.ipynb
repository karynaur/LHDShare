{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-EI-vgsrw6J",
        "outputId": "3fe69b09-a983-45fd-faa2-56400d3b8b5b"
      },
      "source": [
        "!git clone https://github.com/huggingface/transfer-learning-conv-ai\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transfer-learning-conv-ai'...\n",
            "remote: Enumerating objects: 132, done.\u001b[K\n",
            "remote: Total 132 (delta 0), reused 0 (delta 0), pack-reused 132\u001b[K\n",
            "Receiving objects: 100% (132/132), 56.09 KiB | 1.27 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.2.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xyg4rP0PsB9M",
        "outputId": "51fb615f-14c2-4070-817d-d02873cfedf4"
      },
      "source": [
        "%cd transfer-learning-conv-ai/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transfer-learning-conv-ai\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xas8kpRusJdh",
        "outputId": "5633695c-1167-423f-8110-fff6b4ffa9ef"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.8.1+cu101)\n",
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/d3/640f70d69393b415e6a29b27c735047ad86267921ad62682d1d756556d48/pytorch_ignite-0.4.4-py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 5.9MB/s \n",
            "\u001b[?25hCollecting transformers==2.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 6.7MB/s \n",
            "\u001b[?25hCollecting tensorboardX==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (2.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 3)) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 9.7MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/10/a997a266165e2df1976c4fc973f71bcd2e65a255f92d0ff7ab59b2f81989/boto3-1.17.44-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 3)) (2.23.0)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 12.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 39.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1->-r requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->-r requirements.txt (line 4)) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (2.4.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (2.4.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.6.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.32.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.1.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.12.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (2.10.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.12.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.36.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (3.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->-r requirements.txt (line 5)) (0.3.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 6)) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 6)) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 6)) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 6)) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 6)) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 6)) (54.2.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 6)) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 6)) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->-r requirements.txt (line 6)) (1.0.5)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/14/0b4be62b65c52d6d1c442f24e02d2a9889a73d3c352002e14c70f84a679f/s3transfer-0.3.6-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.8MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.44\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/80/3ddbe4ad2561804b887deb8072d802dc24dd759833139a5b91efcff308d6/botocore-1.20.44-py2.py3-none-any.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 45.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.1->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.1->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements.txt (line 5)) (1.28.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements.txt (line 5)) (0.4.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements.txt (line 5)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow->-r requirements.txt (line 5)) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 6)) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.44->boto3->transformers==2.5.1->-r requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements.txt (line 5)) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements.txt (line 5)) (4.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->-r requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->-r requirements.txt (line 5)) (3.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=5a2d5cc82d4900ffa95dcd6e9a249839fb1900c68c378d4afacda934ec75fde6\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "\u001b[31mERROR: botocore 1.20.44 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pytorch-ignite, sentencepiece, jmespath, botocore, s3transfer, boto3, tokenizers, sacremoses, transformers, tensorboardX\n",
            "Successfully installed boto3-1.17.44 botocore-1.20.44 jmespath-0.10.0 pytorch-ignite-0.4.4 s3transfer-0.3.6 sacremoses-0.0.43 sentencepiece-0.1.95 tensorboardX-1.8 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBsWfvHPr6uj",
        "outputId": "91958838-3c5e-4adb-983c-3960f24be229"
      },
      "source": [
        "\n",
        "import logging\n",
        "import random\n",
        "from argparse import ArgumentParser\n",
        "from itertools import chain\n",
        "from pprint import pformat\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from train import SPECIAL_TOKENS, build_input_from_segments, add_special_tokens_\n",
        "from utils import get_dataset, download_pretrained_model\n",
        "\n",
        "def top_filtering(logits, top_k=0., top_p=0.9, threshold=-float('Inf'), filter_value=-float('Inf')):\n",
        "\n",
        "    assert logits.dim() == 1  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
        "    top_k = min(top_k, logits.size(-1))\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token in the top-k tokens\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        # Compute cumulative probabilities of sorted tokens\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # Back to unsorted indices and set them to -infinity\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    indices_to_remove = logits < threshold\n",
        "    logits[indices_to_remove] = filter_value\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(personality, history, tokenizer, model, args, current_output=None):\n",
        "    special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
        "    if current_output is None:\n",
        "        current_output = []\n",
        "\n",
        "    for i in range(args.max_length):\n",
        "        instance = build_input_from_segments(personality, history, current_output, tokenizer, with_eos=False)\n",
        "\n",
        "        input_ids = torch.tensor(instance[\"input_ids\"], device=args.device).unsqueeze(0)\n",
        "        token_type_ids = torch.tensor(instance[\"token_type_ids\"], device=args.device).unsqueeze(0)\n",
        "\n",
        "        logits = model(input_ids, token_type_ids=token_type_ids)\n",
        "        if isinstance(logits, tuple):  # for gpt2 and maybe others\n",
        "            logits = logits[0]\n",
        "        logits = logits[0, -1, :] / args.temperature\n",
        "        logits = top_filtering(logits, top_k=args.top_k, top_p=args.top_p)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        prev = torch.topk(probs, 1)[1] if args.no_sample else torch.multinomial(probs, 1)\n",
        "        if i < args.min_length and prev.item() in special_tokens_ids:\n",
        "            while prev.item() in special_tokens_ids:\n",
        "                if probs.max().item() == 1:\n",
        "                    warnings.warn(\"Warning: model generating special token with probability 1.\")\n",
        "                    break  # avoid infinitely looping over special token\n",
        "                prev = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        if prev.item() in special_tokens_ids:\n",
        "            break\n",
        "        current_output.append(prev.item())\n",
        "\n",
        "    return current_output\n",
        "\n",
        "def run():\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument(\"--dataset_path\", type=str, default=\"\", help=\"Path or url of the dataset. If empty download from S3.\")\n",
        "    parser.add_argument(\"--dataset_cache\", type=str, default='./dataset_cache', help=\"Path or url of the dataset cache\")\n",
        "    parser.add_argument(\"--model\", type=str, default=\"openai-gpt\", help=\"Model type (openai-gpt or gpt2)\", choices=['openai-gpt', 'gpt2'])  # anything besides gpt2 will load openai-gpt\n",
        "    parser.add_argument(\"--model_checkpoint\", type=str, default=\"\", help=\"Path, url or short name of the model\")\n",
        "    parser.add_argument(\"--max_history\", type=int, default=2, help=\"Number of previous utterances to keep in history\")\n",
        "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"Device (cuda or cpu)\")\n",
        "\n",
        "    parser.add_argument(\"--no_sample\", action='store_true', help=\"Set to use greedy decoding instead of sampling\")\n",
        "    parser.add_argument(\"--max_length\", type=int, default=20, help=\"Maximum length of the output utterances\")\n",
        "    parser.add_argument(\"--min_length\", type=int, default=1, help=\"Minimum length of the output utterances\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=0, help=\"Seed\")\n",
        "    parser.add_argument(\"--temperature\", type=float, default=0.7, help=\"Sampling softmax temperature\")\n",
        "    parser.add_argument(\"--top_k\", type=int, default=0, help=\"Filter top-k tokens before sampling (<=0: no filtering)\")\n",
        "    parser.add_argument(\"--top_p\", type=float, default=0.9, help=\"Nucleus filtering (top-p) before sampling (<=0.0: no filtering)\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    logger = logging.getLogger(__file__)\n",
        "    logger.info(pformat(args))\n",
        "\n",
        "    if args.model_checkpoint == \"\":\n",
        "        if args.model == 'gpt2':\n",
        "            raise ValueError(\"Interacting with GPT2 requires passing a finetuned model_checkpoint\")\n",
        "        else:\n",
        "            args.model_checkpoint = download_pretrained_model()\n",
        "\t\n",
        "\t\n",
        "    if args.seed != 0:\n",
        "    \trandom.seed(args.seed)\n",
        "    \ttorch.random.manual_seed(args.seed)\n",
        "    \ttorch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "    logger.info(\"Get pretrained model and tokenizer\")\n",
        "    tokenizer_class, model_class = (GPT2Tokenizer, GPT2LMHeadModel) if args.model == 'gpt2' else (OpenAIGPTTokenizer, OpenAIGPTLMHeadModel)\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.model_checkpoint)\n",
        "    model = model_class.from_pretrained(args.model_checkpoint)\n",
        "    model.to(args.device)\n",
        "    add_special_tokens_(model, tokenizer)\n",
        "\n",
        "    logger.info(\"Sample a personality\")\n",
        "    dataset = get_dataset(tokenizer, args.dataset_path, args.dataset_cache)\n",
        "    personalities = [dialog[\"personality\"] for dataset in dataset.values() for dialog in dataset]\n",
        "    personality = random.choice(personalities)\n",
        "    logger.info(\"Selected personality: %s\", tokenizer.decode(chain(*personality)))\n",
        "\n",
        "    history = []\n",
        "    while True:\n",
        "        raw_text = input(\">>> \")\n",
        "        while not raw_text:\n",
        "            print('Prompt should not be empty!')\n",
        "            raw_text = input(\">>> \")\n",
        "        history.append(tokenizer.encode(raw_text))\n",
        "        with torch.no_grad():\n",
        "            out_ids = sample_sequence(personality, history, tokenizer, model, args)\n",
        "        history.append(out_ids)\n",
        "        history = history[-(2*args.max_history+1):]\n",
        "        out_text = tokenizer.decode(out_ids, skip_special_tokens=True)\n",
        "        print(out_text)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-03 07:57:30.751403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "INFO:interact.py:Namespace(dataset_cache='./dataset_cache', dataset_path='', device='cpu', max_history=2, max_length=20, min_length=1, model='openai-gpt', model_checkpoint='', no_sample=False, seed=0, temperature=0.7, top_k=0, top_p=0.9)\n",
            "INFO:/content/transfer-learning-conv-ai/utils.py:extracting archive file /root/.cache/torch/transformers/2f5114b5eb72f9515802779c42c1b289bebdb1cfc8ce94c653237518eb530b34.75f2a4fe69178ff43138117a977e107a5fc7d402603a0825a296b531f246b3f2 to temp dir /tmp/tmp9btj9psy\n",
            "INFO:interact.py:Get pretrained model and tokenizer\n",
            "INFO:transformers.tokenization_utils:Model name '/tmp/tmp9btj9psy' not found in model shortcut name list (openai-gpt). Assuming '/tmp/tmp9btj9psy' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "INFO:transformers.tokenization_utils:Didn't find file /tmp/tmp9btj9psy/special_tokens_map.json. We won't load it.\n",
            "INFO:transformers.tokenization_utils:Didn't find file /tmp/tmp9btj9psy/tokenizer_config.json. We won't load it.\n",
            "INFO:transformers.tokenization_utils:loading file /tmp/tmp9btj9psy/vocab.json\n",
            "INFO:transformers.tokenization_utils:loading file /tmp/tmp9btj9psy/merges.txt\n",
            "INFO:transformers.tokenization_utils:loading file /tmp/tmp9btj9psy/added_tokens.json\n",
            "INFO:transformers.tokenization_utils:loading file None\n",
            "INFO:transformers.tokenization_utils:loading file None\n",
            "WARNING:transformers.tokenization_openai:ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n",
            "INFO:transformers.configuration_utils:loading configuration file /tmp/tmp9btj9psy/config.json\n",
            "INFO:transformers.configuration_utils:Model config OpenAIGPTConfig {\n",
            "  \"afn\": \"gelu\",\n",
            "  \"architectures\": null,\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"openai-gpt\",\n",
            "  \"n_ctx\": 512,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 512,\n",
            "  \"n_special\": 5,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 40483\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file /tmp/tmp9btj9psy/pytorch_model.bin\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in OpenAIGPTLMHeadModel: ['multiple_choice_head.summary.weight', 'multiple_choice_head.summary.bias']\n",
            "INFO:transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\n",
            "INFO:transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\n",
            "INFO:transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\n",
            "INFO:transformers.tokenization_utils:Assigning ['<speaker1>', '<speaker2>'] to the additional_special_tokens key of the tokenizer\n",
            "INFO:interact.py:Sample a personality\n",
            "INFO:/content/transfer-learning-conv-ai/utils.py:Download dataset from https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\n",
            "INFO:/content/transfer-learning-conv-ai/utils.py:Tokenize and encode the dataset\n",
            "INFO:interact.py:Selected personality: i give lessons on string instruments. i like to cook with food i grow in my garden. i play many instruments. my favorite music genre is classical. i like to travel.\n",
            ">>> hi\n",
            "hey, how are you? i am doing great.\n",
            ">>> Are you participating in LHD Share?\n",
            "no, but i do love playing music. what do you like to do?\n",
            ">>> OH i love participating in MLH events\n",
            "where are you from? i'm from georgia.\n",
            ">>> From India!\n",
            "i live in the usa\n",
            ">>> Oh i see\n",
            "my favorite country is the french.\n",
            ">>> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcNHkLS8sEzQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}